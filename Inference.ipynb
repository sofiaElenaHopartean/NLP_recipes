{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Inference.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "196prI5NszpW-xwmru0AEm48r9CaJ3FNI",
      "authorship_tag": "ABX9TyOHkXuM7ZSu6INKVFM1Q8kj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sofiaElenaHopartean/NLP_recipes/blob/main/Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7UUKvJPNjRL"
      },
      "source": [
        "import sys\n",
        "import warnings\n",
        "import unicodedata\n",
        "import re\n",
        "import random\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import load_model\n",
        "import tensorflow as tf\n",
        "from functools import partial\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFwsr0CCP0UM"
      },
      "source": [
        "params={\n",
        "    'save_dir': 'drive/MyDrive/recipes/saved_model_tf/model4/',\n",
        "    'padding_size': 150,\n",
        "    'num_classes' : 5\n",
        "}\n",
        "colname1, colname2 = 'norm_directions', 'norm_ingredients'"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjvT3g9jPHJs"
      },
      "source": [
        "def load_obj(directory, name):\n",
        "  '''Helper function using pickle to save and load objects'''\n",
        "  with open(directory + name + \".pkl\", \"rb\") as f:\n",
        "      return pickle.load(f)\n",
        "\n",
        "\n",
        "word_index = load_obj(params['save_dir'], \"word_index\")\n",
        "word_lookup = load_obj(params['save_dir'], \"word_lookup\")\n",
        "labels_index = load_obj(params['save_dir'], \"labels_index\")\n",
        "labels_lookup = load_obj(params['save_dir'], \"labels_lookup\")\n",
        "tokenizer = load_obj(params['save_dir'], \"tokenizer\")\n",
        "sp_chrs = load_obj(params['save_dir'], \"sp_chr\")\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2pQb3BVM9SP"
      },
      "source": [
        "### Rebuild test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bak96OXz9XO5"
      },
      "source": [
        "recipes_df = pd.read_csv(\"drive/MyDrive/recipes/recipes.csv\", delimiter=\";\")\n",
        "recipes_df_small = recipes_df[~recipes_df['Directions'].isna()][[\"Directions\", \"Ingredients\"]]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McbChSWl9dun"
      },
      "source": [
        "def preprocess_dirs(text):\n",
        "  _dirs = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode(\"utf-8\")\n",
        "  for sp_chr in sp_chrs: \n",
        "    _dirs = _dirs.replace(sp_chr, \" \")\n",
        "  _dirs = re.sub('\\d', '', _dirs)\n",
        "  return ' '.join(_dirs.split())\n",
        "\n",
        "def preprocess_ingred(text):\n",
        "  _ingreds = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode(\"utf-8\")\n",
        "  for sp_chr in sp_chrs: \n",
        "    if sp_chr != \",\":\n",
        "      _ingreds = _ingreds.replace(sp_chr, \" \")\n",
        "  _ingreds = re.sub('\\d', '', _ingreds)\n",
        "  return ' '.join(_ingreds.split())\n",
        "\n",
        "recipes_df_small[\"lo_directions\"] = recipes_df_small[\"Directions\"].str.lower()\n",
        "recipes_df_small[\"lo_ingredients\"] = recipes_df_small[\"Ingredients\"].str.lower()\n",
        "\n",
        "recipes_df_small[\"norm_directions\"] = recipes_df_small[\"lo_directions\"].apply(preprocess_dirs)\n",
        "recipes_df_small[\"norm_ingredients\"] = recipes_df_small[\"lo_ingredients\"].apply(preprocess_ingred)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "II_HQo77NBDT"
      },
      "source": [
        "X, y = [], []\n",
        "for index, line in recipes_df_small[[colname1, colname2]].iterrows():\n",
        "  _descr = line[colname1]\n",
        "  _ingreds = re.split(',\\s*', line[colname2])\n",
        "  for ingred in _ingreds:\n",
        "    replc = \"S-INGREDIENT\"\n",
        "    parts = ingred.split(\" \")\n",
        "    if len(parts)==1: replc = \"S-INGREDIENT\"\n",
        "    elif len(parts)==2: replc = \"B-INGREDIENT E-INGREDIENT\"\n",
        "    else: \n",
        "      replc = \"B-INGREDIENT \" +\" \".join([\"I-INGREDIENT\"]*(len(parts)-2)) +\" E-INGREDIENT\"\n",
        "    \n",
        "    if ingred in _descr:\n",
        "      _descr = _descr.replace(ingred, replc)\n",
        "  _descr = _descr.split()\n",
        "  _labels = [ w if w in labels_index.keys() else \"O\" for w in _descr]\n",
        "  \n",
        "  words = [word_index.get(x) if x in word_index.keys() else 0 \n",
        "            for x in line[colname1].split()]\n",
        "  labels = [labels_index.get(y) for y in _labels]\n",
        "  X.append(words)\n",
        "  y.append(labels)\n",
        "\n",
        "X = pad_sequences(X, maxlen = params['padding_size'], value = word_index[\"ENDPAD\"], padding = \"post\")\n",
        "y = pad_sequences(y, maxlen = params['padding_size'], value = labels_index[\"O\"], padding = \"post\")\n",
        "y_cat = [to_categorical(i, num_classes = params['num_classes']) for i in y]\n",
        "\n",
        "_, X_test, _, y_test = train_test_split(X,y_cat, train_size=0.9, random_state= 11 )"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSPS2yhxQ7TQ"
      },
      "source": [
        "### Evaluate Model on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Efsld4WLRGte",
        "outputId": "e4a71857-7c82-4c75-b734-f074a3fae827"
      },
      "source": [
        "reconstructed_model = load_model(params['save_dir'])\n",
        "reconstructed_model.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 150)]             0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 150, 150)          805200    \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d (SpatialDr (None, 150, 150)          0         \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 150, 300)          361200    \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 150, 5)            1505      \n",
            "=================================================================\n",
            "Total params: 1,167,905\n",
            "Trainable params: 1,167,905\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDGUkuRsNA8g",
        "outputId": "10add80b-8a70-42ff-ee8b-b9de7e60e915"
      },
      "source": [
        "metrics = reconstructed_model.evaluate(X_test, np.array(y_test))\n",
        "print(metrics)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39/39 [==============================] - 5s 135ms/step - loss: 0.0240 - accuracy: 0.9909\n",
            "[0.023968150839209557, 0.9909257888793945]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwU0B1uU9sXd",
        "outputId": "148e6029-4ca3-40bf-bb82-fd2ae7190083"
      },
      "source": [
        "input = {\n",
        "  \"recipe1\": \"Spray bread machine pan with vegetable oil spray.**Premix ingredients in order listed. Place mixture in bread machine pan.**Select the Quick Bread/Cake cycle.  Press Start. Check after 1 minute to see if mixture is well blended.**Cook until cake cycle stops.  Remove pan, and cool completely before removing bread from pan.**\",\n",
        "  \"recipe2\": \"Preheat oven to 350 degrees F (175 degrees C). Grease and flour a 9x5 inch loaf pan. In a medium bowl, mix flour, soda, salt, 3/4 teaspoon nutmeg, ginger and cloves. Set aside.**In a large bowl, cream butter and sugar until light and fluffy. Beat in the eggs. Add flour mixture alternately with pumpkin. Stir in chocolate chips and 1/2 cup of the walnuts. Pour batter into loaf pan. Sprinkle remaining nuts on top.**Bake at 350 degrees F (175 degrees C) for 65 to 70 minutes or until toothpick inserted into center of cake comes out clean. While still warm, drizzle with glaze. Cool for 6 hours before serving.**to make the glaze:  In a medium bowl, combine confectioners sugar, nutmeg and cinnamon. Mix and add 1 to 2 teaspoons cream until drizzling consistency.**\"\n",
        "}\n",
        "\n",
        "def inference(input):\n",
        "  recipes=[]\n",
        "  delimiters=[]\n",
        "  keys = list(input.keys())\n",
        "  for name in keys:\n",
        "    _dir = preprocess_dirs(input[name]).split()\n",
        "    recipes.append([ word_index.get(x) if x in word_index.keys() else 0 \n",
        "                    for x in _dir])\n",
        "    pos_sum = 0\n",
        "    pos = []\n",
        "    for x in _dir: \n",
        "      pos_sum += len(x) + 1\n",
        "      pos.append(pos_sum)\n",
        "    delimiters.append(pos)\n",
        "\n",
        "  X = pad_sequences(recipes, maxlen = params['padding_size'], value = word_index[\"ENDPAD\"], padding = \"post\")\n",
        "  p = reconstructed_model.predict(np.array(X))\n",
        "  p = np.argmax(p, axis=-1)\n",
        "  # p.shape\n",
        "  # print(delimiters[0])\n",
        "  # print(delimiters[1])\n",
        "\n",
        "  output={}\n",
        "  for i in range(len(keys)):\n",
        "    _res = []\n",
        "    j = 0\n",
        "    ingred = \"\"\n",
        "    start, end = 0,0\n",
        "    while j < len(p[i]):\n",
        "      ner_tok = p[i][j]\n",
        "      if ner_tok == 1: \n",
        "        _res.append([ word_lookup[X[i][j]],\n",
        "                    0 if j == 0 else delimiters[i][j-1] +1, delimiters[i][j]])\n",
        "      if ner_tok > 1:\n",
        "        ingred+= word_lookup[X[i][j]]\n",
        "        start = 0 if j == 0 else delimiters[i][j-1] +1 \n",
        "        end = delimiters[i][j]\n",
        "      if ner_tok == 0 and ingred != \"\":\n",
        "        _res.append([ word_lookup[X[i][j]],\n",
        "                    start, end])\n",
        "        ingred = \"\"\n",
        "        start, end = 0,0\n",
        "      j+= 1\n",
        "    output[keys[i]] = _res\n",
        "  return output\n",
        "\n",
        "inference(input)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'recipe1': [['bread', 7, 12], ['bread', 102, 107], ['bread', 294, 299]],\n",
              " 'recipe2': [['flour', 48, 53],\n",
              "  ['flour', 93, 98],\n",
              "  ['salt', 104, 108],\n",
              "  ['nutmeg', 118, 124],\n",
              "  ['ginger', 125, 131],\n",
              "  ['butter', 175, 181],\n",
              "  ['flour', 236, 241],\n",
              "  ['pumpkin', 267, 274],\n",
              "  ['chocolate', 283, 292],\n",
              "  ['sugar', 609, 614],\n",
              "  ['nutmeg', 615, 621],\n",
              "  ['cinnamon', 626, 634]]}"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gBX4LROVDBi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}